{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indie-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/hungry_geese/hungry_geese.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subsequent-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "import base64\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('trial-9800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tamil-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'w' (str) to file 'submission.py'.\n"
     ]
    }
   ],
   "source": [
    "weight_base64 = base64.b64encode(bz2.compress(pickle.dumps(model.get_weights())))\n",
    "w = \"weight= %s\"%weight_base64\n",
    "%store w >submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "round-victim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a submission.py\n",
    "import pickle\n",
    "import base64\n",
    "import bz2\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, \\\n",
    "                                                                row_col, adjacent_positions, translate, min_distance\n",
    "\n",
    "from kaggle_environments import make\n",
    "from random import choice\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def geese_heads(obs_dict, config_dict):\n",
    "    \"\"\"\n",
    "    Return the position of the geese's heads\n",
    "    \"\"\"\n",
    "    configuration = Configuration(config_dict)\n",
    "\n",
    "    observation = Observation(obs_dict)\n",
    "    player_index = observation.index\n",
    "    player_goose = observation.geese[player_index]\n",
    "    player_head = player_goose[0]\n",
    "    player_row, player_column = row_col(player_head, configuration.columns)\n",
    "    positions = []\n",
    "    for geese in observation.geese:\n",
    "        if len(geese)>0:\n",
    "            geese_head = geese[0]\n",
    "            row, column = row_col(geese_head, configuration.columns)\n",
    "        else:\n",
    "            row = None\n",
    "            column = None\n",
    "        positions.append((row, column))\n",
    "    return positions\n",
    "\n",
    "def get_last_actions(previous_geese_heads, heads_positions):\n",
    "\n",
    "    def get_last_action(prev, cur):\n",
    "        last_action = None\n",
    "\n",
    "        prev_row = prev[0]\n",
    "        prev_col = prev[1]\n",
    "        cur_row = cur[0]\n",
    "        cur_col = cur[1]\n",
    "\n",
    "        if cur_row is not None:\n",
    "            if (cur_row-prev_row == 1) | ((cur_row==0) & (prev_row==6)):\n",
    "                last_action = Action.SOUTH.name\n",
    "            elif (cur_row-prev_row == -1) | ((cur_row==6) & (prev_row==0)):\n",
    "                last_action = Action.NORTH.name\n",
    "            elif (cur_col-prev_col == 1) | ((cur_col==0) & (prev_col==10)):\n",
    "                last_action = Action.EAST.name\n",
    "            elif (cur_col-prev_col == -1) | ((cur_col==10) & (prev_col==0)):\n",
    "                last_action = Action.WEST.name\n",
    "\n",
    "        return last_action\n",
    "\n",
    "    if len(previous_geese_heads) == 0:\n",
    "        actions = [Action.SOUTH.name, Action.NORTH.name, Action.EAST.name, Action.WEST.name]\n",
    "        nb_geeses = len(heads_positions)\n",
    "        last_actions = [actions[np.random.randint(4)] for _ in range(nb_geeses)]\n",
    "    else:   \n",
    "        last_actions = [get_last_action(*pos) for pos in zip(previous_geese_heads, heads_positions)]\n",
    "\n",
    "    return last_actions\n",
    "    \n",
    "def central_state_space(obs_dict, config_dict, prev_head):\n",
    "    \"\"\"\n",
    "    Recreating a board where my agent's head in the middle of the board \n",
    "    (position (4,5)), and creating features accordingly\n",
    "    \"\"\"\n",
    "    \n",
    "    configuration = Configuration(config_dict)\n",
    "\n",
    "    observation = Observation(obs_dict)\n",
    "    player_index = observation.index\n",
    "    player_goose = observation.geese[player_index]\n",
    "    if len(player_goose)==0:\n",
    "        player_head = prev_head\n",
    "    else:\n",
    "        player_head = player_goose[0]\n",
    "    player_row, player_column = row_col(player_head, configuration.columns)\n",
    "    row_offset = player_row - 3\n",
    "    column_offset = player_row - 5\n",
    "\n",
    "    foods = observation['food']\n",
    "\n",
    "    def centralize(row, col):\n",
    "        if col > player_column:\n",
    "            new_col = (5 + col - player_column) % 11\n",
    "        else:\n",
    "            new_col = 5 - (player_column - col)\n",
    "            if new_col < 0:\n",
    "                new_col += 11\n",
    "\n",
    "        if row > player_row:\n",
    "            new_row = (3 + row - player_row) % 7\n",
    "        else:\n",
    "            new_row = 3 - (player_row - row)\n",
    "            if new_row < 0:\n",
    "                new_row += 7\n",
    "        return new_row, new_col\n",
    "\n",
    "    food1_row, food1_column = centralize(*row_col(foods[0], configuration.columns))\n",
    "    food2_row, food2_column = centralize(*row_col(foods[1], configuration.columns))\n",
    "\n",
    "    food1_row_feat = float(food1_row - 3)/3 if food1_row>=3 else float(food1_row - 3)/3\n",
    "    food2_row_feat = float(food2_row - 3)/3 if food2_row>=3 else float(food2_row - 3)/3\n",
    "\n",
    "    food1_col_feat = float(food1_column - 5)/5 if food1_column>=5 else float(food1_column - 5)/5\n",
    "    food2_col_feat = float(food2_column - 5)/5 if food2_column>=5 else float(food2_column - 5)/5\n",
    "    food_feats = np.array([food1_row_feat, food2_row_feat, food1_col_feat, food2_col_feat])\n",
    "\n",
    "    # Create the grid\n",
    "    board = np.zeros([7, 11])\n",
    "    # Add food to board\n",
    "#     board[food1_row, food1_column] = 1\n",
    "#     board[food2_row, food2_column] = 1\n",
    "\n",
    "    for ind, goose in enumerate(observation.geese):\n",
    "        if ind!= player_index:\n",
    "            if len(goose)>0:\n",
    "\n",
    "                ap = adjacent_positions(goose[0], 11, 7)\n",
    "                for p in ap:\n",
    "                    row, col = centralize(*row_col(p, configuration.columns))    \n",
    "                    board[row,col]=-.33             \n",
    "\n",
    "        for pos in goose:\n",
    "            if len(goose)>0:\n",
    "                # set bodies to 1\n",
    "                row, col = centralize(*row_col(pos, configuration.columns))\n",
    "                board[row, col]=-1\n",
    "                \n",
    "   \n",
    "        if len(goose)>1:\n",
    "            # Set tails to -0.1 if the goose has a tail\n",
    "            row, col = centralize(*row_col(goose[-1], configuration.columns))\n",
    "            board[row, col]=-0.1    \n",
    "                \n",
    "    board[3, 5] = 0\n",
    "    return board, food_feats\n",
    "\n",
    "def min_dir(p1, p2, max_p):\n",
    "    \"\"\"\n",
    "    min distance and direction from p1\n",
    "    \"\"\"\n",
    "    direction = 'left' # left by default\n",
    "    d1 = abs(p1 - p2) # Distance going across board\n",
    "    d2 = min(abs(p1-0), abs(p1 - max_p)) + min(abs(p2-0), abs(p2 - max_p)) # Distance wrapping around board\n",
    "    \n",
    "    if p1>p2 and d1<d2:\n",
    "        direction = 'left'\n",
    "        \n",
    "    elif p1>p2 and d1>d2:\n",
    "        direction ='right'\n",
    "    \n",
    "    elif p2>p1 and d1<d2:\n",
    "        direction = 'right'\n",
    "    \n",
    "    elif p2>p1 and d1>d2:\n",
    "        direction = 'left'\n",
    "    \n",
    "    dir_vec = np.zeros(2)\n",
    "        \n",
    "    if direction=='left':\n",
    "        dir_vec[0]=1\n",
    "    \n",
    "    else:\n",
    "        dir_vec[1]=1\n",
    "    \n",
    "    min_dist = np.array([min(d1, d2)])/max_p\n",
    "    return dir_vec, min_dist\n",
    "        \n",
    "\n",
    "class StateTranslator_Central:\n",
    "    \"\"\"\n",
    "    Returns a board where we are always at the center\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.last_action = None\n",
    "        self.step_count = 0\n",
    "        self.last_goose_length = 1\n",
    "        self.last_goose_ind = 0\n",
    "        self.observations = []\n",
    "        \n",
    "    def set_last_action(self, last_action):\n",
    "        self.last_action = last_action\n",
    "        \n",
    "        \n",
    "    def __get_last_action_vec(self):\n",
    "        action_vec = np.zeros(4)\n",
    "        \n",
    "        if self.last_action == 'NORTH':\n",
    "            action_vec[0] = 1\n",
    "        elif self.last_action == 'SOUTH':\n",
    "            action_vec[1] = 1\n",
    "        elif self.last_action == 'EAST':\n",
    "            action_vec[2] = 1\n",
    "        elif self.last_action == 'WEST':\n",
    "            action_vec[3] = 1\n",
    "        \n",
    "        return action_vec\n",
    "    \n",
    "    def translate_action_to_text(self, action):\n",
    "        \n",
    "        h = {0 : 'WEST',\n",
    "             1: 'EAST',\n",
    "             2: 'NORTH',\n",
    "             3: 'SOUTH'}\n",
    "        \n",
    "        return h[action]\n",
    "\n",
    "    def translate_text_to_int(self, action):\n",
    "        h = {'WEST':0,\n",
    "            'EAST':1,\n",
    "            'NORTH':2,\n",
    "            'SOUTH':3}\n",
    "        \n",
    "        return h[action]\n",
    "    \n",
    "    \n",
    "    def update_length(self):\n",
    "        self.last_goose_length = self.current_goose_length\n",
    "        \n",
    "    \n",
    "    def get_state(self, observation, config):\n",
    "        \n",
    "        #### This is exception handling for if our goose died this turn, to use the last\n",
    "        ### known index as its postion for the state centralizer\n",
    "        geese = observation['geese']\n",
    "        if len(geese[observation['index']])>0:\n",
    "            self.last_goose_ind = geese[observation['index']][0]\n",
    "            self.my_goose = geese[observation['index']][0]\n",
    "            \n",
    "        \n",
    "        board, food_feats = central_state_space(observation, config, self.last_goose_ind)\n",
    "        self.step_count = observation['step']\n",
    "        \n",
    "        \n",
    "        self.current_goose_length = len(geese[observation['index']])  \n",
    "        \n",
    "        \n",
    "        food = observation['food']\n",
    "        dir_vec1x, min_dist1x = min_dir(self.my_goose, food[0] , 11)\n",
    "        dir_vec1y, min_dist1y = min_dir(self.my_goose, food[0] , 7)\n",
    "        \n",
    "        dir_vec2x, min_dist2x = min_dir(self.my_goose, food[1] , 11)   \n",
    "        dir_vec2y, min_dist2y = min_dir(self.my_goose, food[1] , 7)  \n",
    "        food_vec = np.concatenate((dir_vec1x, dir_vec1y, dir_vec2x, dir_vec2y,\n",
    "                            min_dist1x, min_dist1y, min_dist2x, min_dist2y))\n",
    "        ####\n",
    "        biggest_goose = 0\n",
    "        alive_geese = 0\n",
    "        for ind, goose in enumerate(geese):\n",
    "            if len(goose)>biggest_goose:\n",
    "                biggest_goose = len(goose)\n",
    "            if ind != observation['index'] and len(goose)>0:\n",
    "                alive_geese+=1\n",
    "        \n",
    "        state = np.array([])\n",
    "        state = np.append(state, self.__get_last_action_vec())\n",
    "        state = np.append(state, self.current_goose_length/15)\n",
    "        state = np.append(state, biggest_goose/15)\n",
    "        state = np.append(state, alive_geese/4)\n",
    "        state = np.append(state, self.step_count/200)\n",
    "        state = np.append(state, food_vec)\n",
    "        state = np.append(state, board.flatten())\n",
    "        \n",
    "        state = state.flatten()\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def calculate_reward(self, observation):\n",
    "        \n",
    "        current_geese = observation['geese']\n",
    "        prev = self.last_goose_length\n",
    "        cur = len(current_geese[observation['index']])  \n",
    "        \n",
    "        reward = -1       \n",
    "    \n",
    "        ### If we grow, reward is 100\n",
    "        if cur>prev:\n",
    "            reward = 100\n",
    "        \n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        ### If we die -150\n",
    "        if cur == 0:\n",
    "            reward = -150\n",
    "            \n",
    "        ### see if any geese are alive\n",
    "\n",
    "        alive_geese = 0\n",
    "        for ind, goose in enumerate(current_geese):\n",
    "            if ind != observation['index'] and len(goose)>0:\n",
    "                alive_geese+=1\n",
    "        \n",
    "        # If we are the last one standing\n",
    "        if alive_geese == 0 and cur>0:\n",
    "            reward = 500\n",
    "            \n",
    "        ### if the game ends and we are the biggest\n",
    "        if self.step_count == 200:\n",
    "            biggest_goose = 0\n",
    "            biggest_goose_ind = None\n",
    "            for ind, goose in enumerate(current_geese):\n",
    "                if len(goose)>biggest_goose:\n",
    "                    biggest_goose = len(goose)\n",
    "                    biggest_goose_ind = ind\n",
    "            \n",
    "            if biggest_goose_ind == observation['index']:\n",
    "                reward = 500\n",
    "        \n",
    "        return reward\n",
    "\n",
    "class dqnAgent:\n",
    "    \"\"\"\n",
    "    Given an environment state, choose an action, and learn from the reward\n",
    "    https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "    https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc\n",
    "    https://www.researchgate.net/post/What-are-possible-reasons-why-Q-loss-is-not-converging-in-Deep-Q-Learning-algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=None, epsilon = 1.0, epsilon_min = 0.15):\n",
    "\n",
    "        self.StateTrans = StateTranslator_Central()\n",
    "        self.state_shape  = 97\n",
    "        print('my state shape is:', self.state_shape)\n",
    "        self.memory  = deque(maxlen=8000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.001\n",
    "        self.tau = .125\n",
    "\n",
    "        if model == None:\n",
    "            self.model = self.create_model()\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        model.add(Dense(2000, input_dim=self.state_shape, activation=\"relu\"))\n",
    "        model.add(Dense(1000, activation=\"relu\"))\n",
    "        model.add(Dense(500, activation=\"relu\"))\n",
    "        model.add(Dense(1000, activation=\"relu\"))\n",
    "        model.add(Dense(500, activation=\"relu\"))\n",
    "        model.add(Dense(100, activation=\"relu\"))\n",
    "        model.add(Dense(4))\n",
    "        model.compile(loss=\"MSE\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Set random choice to north or east as the agent is not moving in these directions\n",
    "            return random.choice([0,1,2,3])\n",
    "\n",
    "        action_values = self.model.predict(state.reshape(-1, self.state_shape))[0]\n",
    "        action = np.argmax(action_values)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def translate_state(self, observation, configuration):\n",
    "        state = self.StateTrans.get_state(observation, configuration)\n",
    "        return state\n",
    "\n",
    "    def __call__(self, observation, configuration):\n",
    "        \n",
    "        state = self.translate_state(observation, configuration)\n",
    "        action = self.act(state)\n",
    "        # State translator will take in 0, 1, 2, 3 and return straight, left or right, which in turn will \n",
    "        # be translated into a kaggle Action\n",
    "        action_text = self.StateTrans.translate_action_to_text(action)\n",
    "        \n",
    "        # Update our step number and actions\n",
    "        self.StateTrans.set_last_action(action_text)\n",
    "        \n",
    "        return action_text\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        ########################\n",
    "        # This can be sped up significantly, but processing all samples in batch rather than 1 at a time\n",
    "        ####################\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state.reshape(-1, self.state_shape))\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state.reshape(-1, self.state_shape))[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state.reshape(-1, self.state_shape), target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "dqn = dqnAgent(epsilon_min=0,\n",
    "               epsilon=0)\n",
    "\n",
    "dqn.model.set_weights(pickle.loads(bz2.decompress(base64.b64decode(weight))))\n",
    "\n",
    "\n",
    "def agent(observation, config):\n",
    "    \n",
    "    action = dqn(observation, config)\n",
    "    \n",
    "    geese = observation['geese']\n",
    "    \n",
    "    opponents = [\n",
    "            goose\n",
    "            for index, goose in enumerate(geese)\n",
    "            if index != observation.index and len(goose) > 0\n",
    "        ]\n",
    "\n",
    "        \n",
    "    opponent_heads = [opponent[0] for opponent in opponents]\n",
    "        # Don't move adjacent to any heads\n",
    "    head_adjacent_positions = {\n",
    "            opponent_head_adjacent\n",
    "            for opponent_head in opponent_heads\n",
    "            for opponent_head_adjacent in adjacent_positions(opponent_head, columns, rows)\n",
    "        }\n",
    "    \n",
    "    legal_actions = {\n",
    "   \n",
    "        for a in Action\n",
    "        for new_position in [translate(position, a, 11, 7)]\n",
    "        if (\n",
    "            new_position not in head_adjacent_positions and\n",
    "            new_position not in bodies)\n",
    "        }\n",
    "\n",
    "    if action in legal_actions:\n",
    "        return action\n",
    "    \n",
    "    else:\n",
    "        action = choice[actions]\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-wings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "artistic-manitoba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "wicked-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "conditional-exclusive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-nashville",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goose",
   "language": "python",
   "name": "goose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
